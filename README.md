# CapstoneDesign
2022-1 CapstoneDesign  
2022학년도 1학기 소프트웨어융합캡스톤디자인에서 진행된 프로젝트입니다.
## 강화학습을 활용한 운전 시뮬레이션
### 과제 개요
기존 교통 시뮬레이션의 방식은 '시뮬레이션과 관련된 모든 변수를 일일이 코드로 작성하는 방식'입니다. 이를 Rule based Traffic Simulation이라고 합니다.<br>
이러한 Rule based 방식의 경우, 교통 법규를 위반하는 차량을 만들고 싶다면, 위반하도록 코드를 작성해야하고, 갑작스러운 사고 상황에 대처하는 운전자를 만들고자 한다면, 또 그에 맞는 코드를 작성해야합니다. 따라서 **다양한 상황**을 만드는데에는 한계점이 존재합니다.<br>
2020년 발표된 **"Behaviorally Diverse Traffic Simulation via Reinforcement Learning"** 에 따르면, 강화학습이 이러한 한계점을 극복할 수 있다고 합니다.<br>
하지만 강화학습을 활용한 방식의 경우, 주차를 학습한 경우, 주차 상황만을, 주행을 학습했다면 주행 상황만을 시뮬레이션 할 수 있습니다.  
즉, 하나의 학습만으로는 일반적인 상황을 시뮬레이션 할 수 없기 때문에 운신의 폭이 좁습니다.<br>
해당 프로젝트는 강화학습의 이러한 한계점을 개선하고자 다음과 같은 전략을 활용하였습니다.
- 충돌을 피하면서 주어진 목적지에만 도달하는 상황만을 학습한다.
-  길찾기를 별도로 진행하여 Agent에게 최종 목적지까지 도달하는 중간 목적지를 제시한다. 이는 최단 경로 알고리즘을 짤 때 경로를 설계하는 것과 동일합니다. 실제로 길찾기를 진행할 때 A* 알고리즘을 사용합니다.
- 이 때, **경험적으로** 장애물 너머에 있는 목적지는 Agent가 학습을 하지 못하므로, 목적지가 장애물에 가리지 않도록 한다.
- 이러한 방식을 사용한다면, 장애물을 피하면서 중간 목적지에 도달할 것이고, 이를 반복한 결과, 최종 목적지에 도달할 것이다.<br>
이때 Navigator는 기존 Rule based 방식에서 Rule을 담당하는 것이라고 볼 수 있습니다.  
예를 들어 회전 교차로에서 Agent는 오른쪽으로 돌지 왼쪽으로 돌지 알 수 없습니다. 이 때 Navigator가 올바른 방향에 중간 목적지를 위치시키는 것으로 Agent가 법규를 지키게 됩니다.<br>
### 진행
- 강화학습 환경은 Unity와 Unity의 강화학습 관련 공식 패키지인 Unity ML-Agents로 구성하였습니다.
- 사용 Unity 버전 : 2020.3.33f1
- 사용 Unity ML 버전 : Release 19<br>
강화학습은 이번에 처음 배우는 것이라, ""여러 시행착오 끝에"" 다음의 커리큘럼을 구성하였습니다.
1. **머신러닝 관련 공부**<br> 이러나 저러나 머신러닝 + 딥러닝과 연관이 돼 있기 때문에 시작하기 전에 미리 훑어보고 가는 것이 좋습니다.
관련 책으로는 **핸즈온 머신러닝**도 좋습니다만, 머신러닝 공부하느라 강화학습 공부를 못하는 주객전도에 빠질 수 있기 때문에, **혼자 공부하는 머신러닝+딥러닝**도 괜찮습니다. 핸즈온 머신러닝의 요약본 같은 느낌으로 빠르면 이틀 정도 내로 마칠 수 있습니다.<br><br>
2. **혁펜하임의 "트이는" 강화학습**
링크 : https://youtube.com/playlist?list=PL_iJu012NOxehE8fdF9me4TLfbdv3ZW8g <br>
다른 책들의 경우, 강화학습을 어렴풋이, 예시로 설명하지만, 이 강의는 수학적으로 설명합니다. 어차피 실제 강화학습 관련 논문 보면 수식이 난무하기 때문에 예시로 공부하는건 한 두 번이면 충분하고 바로 이 강의 듣는게 정신건강에 이로웠습니다.  
말그대로 수식으로 강화학습의 이론을 전개하기 때문에 보다보면 정신이 아득해지지만, 괜찮습니다. 답도 안 나오는 구글링 백날 하면서 우울해지는 것보다는 정신건강에 훨씬 이로웠습니다.<br>
이번 캡스톤디자인에서 느낀바, 어느정도까지는 구글링으로 많은 것을 해결할 수 있지만, 일정 수준 이후 지식들은 정보의 가뭄입니다. 논문이나 전공 서적 쪽으로 가야합니다. 혁펜하임님처럼 무료로 풀어주시는 분들은 정말 인세 받는 삶 사셔야... 얼른 책 내셨으면 좋을 정도.
확률 및 랜덤변수 정도는 훑고 가는 것이 좋습니다.<br><br>
3. **팡요랩**  
링크 : https://www.youtube.com/channel/UCwkGvF7xKz2E0Lv-fZ9wv2g<br>
혁펜하임님의 강좌로 어느정도 강화학습 관련 수식을 읽을 수 있게 됐다면 이제 강화학습 관련 논문들을 읽을 차례입니다. 이것도 논문이 많아서 엄두가 안 났지만 핵심 논문만 쏙쏙 짚어서 하나하나 정말 하나하나 성실히 설명해주십니다. 그것도 무료로.<br>
책 내셨다길래 당연히 샀습니다.   
판교(Pang-yo) 갈 때마다 혹시나 만나뵐 수 있지 않을까 설레는 마음으로 두리번 거립니다.<br>
이 채널에 올라온 핵심 논문들 영상까지 다 보셨으면 Unity ML-Agents 는 큰 문제 없이 다루실 수 있습니다.<br><br>
4. **Unity ML-Agents 공식 깃허브** 
링크 : https://github.com/Unity-Technologies/ml-agents
<br> 처음에는 공식 문서에 대한 막연한 두려움 때문에 보지 않았었는데, Unity ML-Agents를 설명하는 여러 책과 블로그 글을 보면 너무 오래 된 내용이라 더 이상 써먹기 힘들거나, 알고보면 공식 문서 코드를 그대로 갖다 쓴 경우가 많더군요.<br> 가장 최신에, 가장 정확한 내용이 담기는 곳은 공식 문서입니다.<br> 설명도 세세하고 친절하게 다 돼 있기 때문에 참고하면서 프로젝트 진행하신다면 큰 문제 없을겁니다.

### 결과
![09GitHub](https://user-images.githubusercontent.com/38786000/175095768-46b49815-e7c9-4631-ad98-59bc06d83cda.gif)
GitHub의 파일 용량 제한 때문에 프레임 레이트가 매우 낮습니다.   
또한 학습이 완전히 진행된 상태가 아니기 때문에 처리가 느린 것 또한 확인 할 수 있습니다.   
하지만, 실제 운전하는 것과 같이 주변 장애물들과 거리를 조절하며 목적지까지 다다르는 모습을 확인 할 수 있습니다.

### 결론
장애물을 피하며 목적지까지 도달하는 것을 학습하는 것과 순차적으로 목적지를 Observation으로 제공하는 것을 조합한 방식이 일정 부분 성과를 달성하는 것을 확인할 수 있었습니다.   
다만, 학습이 완전히 진행되지 못했다는 점, 테스트를 진행하는 환경이 하나 뿐이었단 점이 아쉽습니다.   
정확한 평가를 위해, 기준을 정하여, 구체적으로 기존 대비 어느 정도의 수치적 성과가 나오는지, 다양한 환경에서의 검증이 후속 연구로 진행되어야겠습니다. 
